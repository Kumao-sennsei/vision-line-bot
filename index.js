import 'dotenv/config';
import express from 'express';
import { middleware, Client } from '@line/bot-sdk';
import { v2 as cloudinary } from 'cloudinary';
import OpenAI from 'openai';

// LINEè¨­å®š
const lineConfig = {
  channelAccessToken: process.env.LINE_CHANNEL_ACCESS_TOKEN,
  channelSecret: process.env.LINE_CHANNEL_SECRET,
};
const client = new Client(lineConfig);

// Cloudinaryè¨­å®š
cloudinary.config({
  cloud_name: process.env.CLOUDINARY_CLOUD_NAME,
  api_key: process.env.CLOUDINARY_API_KEY,
  api_secret: process.env.CLOUDINARY_API_SECRET,
});

// OpenAIè¨­å®š
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Expressæº–å‚™
const app = express();
app.use(express.json());

// Webhookã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
app.post('/webhook', middleware(lineConfig), async (req, res) => {
  try {
    await Promise.all(req.body.events.map(handleEvent));
    res.status(200).send('OK');
  } catch (e) {
    console.error(e);
    res.status(500).end();
  }
});

// ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†
async function handleEvent(event) {
  // ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  if (event.type === 'message' && event.message.type === 'text') {
    // OpenAI GPTã§æ—¥æœ¬èªè§£èª¬
    try {
      const gptRes = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content:
              'ã‚ãªãŸã¯å„ªã—ãã¦é¢ç™½ã„æ•°å­¦ã®å…ˆç”Ÿã€Œãã¾ãŠå…ˆç”Ÿã€ã§ã™ã€‚å¿…ãšæ—¥æœ¬èªã§ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’äº¤ãˆã¦ã€åˆ†ã‹ã‚Šã‚„ã™ãä¼šè©±ã—ã¦ãã ã•ã„ã€‚',
          },
          {
            role: 'user',
            content: event.message.text,
          },
        ],
        max_tokens: 1000,
      });

      const replyText = gptRes.choices[0].message.content?.trim() || 'ã”ã‚ã‚“ã€ã†ã¾ãèª¬æ˜ã§ããªã‹ã£ãŸã¿ãŸã„â€¦';
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: replyText,
      });
    } catch (e) {
      console.error(e);
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: 'ãã¾ãŠå…ˆç”Ÿã‚‚ã³ã£ãã‚Šï¼ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¡ã‚ƒã„ã¾ã—ãŸâ€¦',
      });
    }
    return;
  }

  // ç”»åƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  if (event.type === 'message' && event.message.type === 'image') {
    try {
      // ç”»åƒãƒãƒƒãƒ•ã‚¡å–å¾—
      const stream = await client.getMessageContent(event.message.id);
      const buffer = await streamToBuffer(stream);

      // Cloudinaryã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
      const uploadResult = await new Promise((resolve, reject) => {
        const uploadStream = cloudinary.uploader.upload_stream(
          { resource_type: 'image' },
          (err, result) => (err ? reject(err) : resolve(result))
        );
        uploadStream.end(buffer);
      });

      // OpenAI Visionã«æ—¥æœ¬èªæŒ‡ç¤ºã§ç”»åƒè§£æ
      const visionRes = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content:
              'ã‚ãªãŸã¯å„ªã—ãã¦é¢ç™½ã„æ•°å­¦ã®å…ˆç”Ÿã€Œãã¾ãŠå…ˆç”Ÿã€ã§ã™ã€‚å¿…ãšæ—¥æœ¬èªã§ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚‚å…¥ã‚Œã¦ã€ç”»åƒã®å†…å®¹ã‚„å•é¡Œã‚’åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚',
          },
          {
            role: 'user',
            content: [
              { type: 'image_url', image_url: { url: uploadResult.secure_url } },
              { type: 'text', text: 'ã“ã®ç”»åƒã®å†…å®¹ã‚’æ—¥æœ¬èªã§è©³ã—ãã€ã‹ã¤é¢ç™½ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚' },
            ],
          },
        ],
        max_tokens: 1000,
      });

      const replyText = visionRes.choices[0].message.content?.trim() || 'ç”»åƒã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸâ€¦';
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: replyText,
      });
    } catch (e) {
      console.error(e);
      await client.replyMessage(event.replyToken, {
        type: 'text',
        text: 'ç”»åƒã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚ˆã€‚ã”ã‚ã‚“ã­ï¼',
      });
    }
    return;
  }
}

// ã‚¹ãƒˆãƒªãƒ¼ãƒ â†’ãƒãƒƒãƒ•ã‚¡å¤‰æ›
function streamToBuffer(stream) {
  return new Promise((resolve, reject) => {
    const chunks = [];
    stream.on('data', (chunk) => chunks.push(chunk));
    stream.on('end', () => resolve(Buffer.concat(chunks)));
    stream.on('error', reject);
  });
}

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`ğŸš€ Server is running on port ${PORT}`));
